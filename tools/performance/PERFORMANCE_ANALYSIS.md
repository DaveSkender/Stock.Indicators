# Performance Analysis: Streaming Indicators

**Last Updated:** January 3, 2026
**Analysis:** Comprehensive performance review of Series, BufferList, and StreamHub implementations

## Executive Summary

Analysis of baseline performance data demonstrates **significant improvements** since October 2025. All O(nÂ²) complexity issues have been resolved, and streaming indicators now achieve acceptable real-time performance for intended use cases.

### Current Performance Status (January 2026)

- **BufferList implementations**: 93% meet acceptable performance targets (<2x overhead)
- **StreamHub implementations**: 53% meet target or acceptable ranges (<3x overhead)
- **Critical O(nÂ²) issues**: 0 remaining (100% fixed)
- **Memory profiling**: Infrastructure ready, compliance validation pending

### Key Improvements Since October 2025

| Metric                    | October 2025  | January 2026   | Improvement    |
| ------------------------- | ------------- | -------------- | -------------- |
| Worst StreamHub slowdown  | 391x (RSI)    | 10.5x (Ema)    | **97% better** |
| Worst BufferList slowdown | 7.85x (Slope) | 3.41x (Slope)  | **57% better** |
| StreamHub avg slowdown    | 28.5x         | ~4.0x          | **86% better** |
| BufferList avg slowdown   | 2.1x          | 1.65x          | **21% better** |
| Critical O(nÂ²) issues     | 4             | 0              | **100% fixed** |

### Performance Targets (NFR-002)

**BufferList:**

- Target: <1.3x overhead (within 30% of Series)
- Acceptable: <2x overhead
- Critical: â‰¥2x overhead requires investigation

**StreamHub:**

- Target: <1.5x overhead (within 50% of Series)
- Acceptable: <3x overhead
- Framework overhead context: 7-11x for EMA/SMA family (~40,000 quotes/second throughput)
- Critical: â‰¥10x overhead indicates algorithmic issues

**Memory:**

- StreamHub: <10KB overhead per instance
- BufferList: <5KB overhead per instance

## BufferList Performance Analysis

### Methodology

- **Dataset**: 502 periods of historical OHLCV data
- **Benchmark Tool**: BenchmarkDotNet with Release configuration
- **Metrics**: Mean execution time (nanoseconds), Error, StdDev, Memory allocations
- **Comparison**: Direct side-by-side execution of Series vs BufferList

### Performance Distribution

**Excellent (<1.3x)**: 55 indicators (~67%)

- Examples: Rsi, Roc, Pmo, Tsi, ChopIndex
- Overhead: <30%
- Status: âœ… Meets target

**Good (1.3x-2x)**: 21 indicators (~26%)

- Examples: Vortex, Prs, Keltner, Obv
- Overhead: 30%-100%
- Status: âœ… Acceptable

**Needs Investigation (â‰¥2x)**: 6 indicators (~7%)

- Status: ðŸ”´ Requires optimization

| Indicator     | Series (ns) | Buffer (ns) | Slowdown  | Priority   |
| ------------- | ----------- | ----------- | --------- | ---------- |
| **Slope**     | 47,859      | 162,972     | **3.41x** | ðŸ”´ HIGH    |
| **Alligator** | 8,609       | 18,570      | **2.16x** | ðŸ”´ MEDIUM  |
| **Adx**       | 15,088      | 31,348      | **2.08x** | ðŸ”´ MEDIUM  |
| **Ema**       | 2,443       | 4,373       | **1.79x** | âš ï¸ REVIEW  |
| **Smma**      | 2,931       | 5,106       | **1.74x** | âš ï¸ REVIEW  |
| **Gator**     | 13,583      | 23,506      | **1.73x** | âš ï¸ REVIEW  |

### BufferList Recommendations

1. **93% of implementations meet performance targets**
2. Focus optimization on 3 critical indicators: Slope, Alligator, Adx
3. Review EMA/SMMA family for potential micro-optimizations
4. All BufferList indicators achieve O(1) incremental updates

## StreamHub Performance Analysis

### Methodology

Same as BufferList analysis:

- **Dataset**: 502 periods fed sequentially through QuoteHub
- **Benchmark Tool**: BenchmarkDotNet with Release configuration
- **Metrics**: Mean execution time (nanoseconds), Error, StdDev, Memory allocations
- **Comparison**: Series batch processing vs StreamHub incremental processing

### Performance Distribution

**Excellent (<1.5x)**: 39 indicators (~47%)

- Examples: Fcb, Prs, Pmo, Tsi, Vortex
- Overhead: <50%
- Status: âœ… Meets target

**Good (1.5x-3x)**: 6 indicators (~7%)

- Examples: Alma, Sma, WilliamsR
- Overhead: 50%-200%
- Status: âœ… Acceptable

**Needs Review (3x-10x)**: 11 indicators (~13%)

- Overhead: 200%-900%
- Status: âš ï¸ Monitor

**Framework Overhead (7-11x)**: 27 indicators (~33%)

- EMA/SMA family with consistent overhead pattern
- Status: â„¹ï¸ Acceptable for intended use cases

### Top StreamHub Issues

| Indicator      | Series (ns) | Stream (ns) | Slowdown    | Category          |
| -------------- | ----------- | ----------- | ----------- | ----------------- |
| **Ema**        | 2,443       | 25,638      | **10.49x**  | EMA overhead      |
| **Pvo**        | 6,187       | 64,756      | **10.47x**  | EMA overhead      |
| **Tema**       | 3,496       | 34,418      | **9.85x**   | EMA overhead      |
| **Smma**       | 2,931       | 25,500      | **8.70x**   | EMA overhead      |
| **T3**         | 4,625       | 39,997      | **8.65x**   | EMA overhead      |
| **Dema**       | 3,545       | 30,349      | **8.56x**   | EMA overhead      |
| **Trix**       | 4,151       | 34,667      | **8.35x**   | EMA overhead      |
| **Awesome**    | 15,533      | 119,340     | **7.68x**   | SMA overhead      |
| **Slope**      | 47,859      | 275,337     | **5.75x**   | Optimized         |
| **Chandelier** | 10,150      | 54,300      | **5.35x**   | Window overhead   |

### Root Cause Analysis

#### Pattern 1: Moving Average Family Framework Overhead (7-11x)

**Affected indicators:**

- EMA-based: Ema, Smma, Tema, Dema, T3, Trix, Pvo, Macd (10.5x, 8.7x, 9.9x, 8.6x, 8.7x, 8.4x, 10.5x, 7.3x)
- SMA-based: Awesome (7.7x)

**Root cause:** StreamHub framework overhead for simple indicators. The EMA/SMA calculation itself is O(1), but the hub subscription/notification infrastructure adds constant overhead that dominates for fast indicators.

**Performance context:** These timings are in nanoseconds. An 8-10x overhead on a 2,500 ns operation equals ~25,000 ns per quote, achieving **~40,000 quotes/second** throughput - adequate for real-time streaming use cases.

**Status:** â„¹ï¸ Acceptable performance for intended use cases. Framework overhead investigation task P001 identified but deferred (see streaming-indicators.plan.md).

#### Pattern 2: Resolved O(nÂ²) Issues

The following critical issues have been **fully resolved**:

| Indicator      | October 2025 | January 2026 | Improvement       |
| -------------- | ------------ | ------------ | ----------------- |
| **Rsi**        | 391.33x      | 3.26x        | **99% faster** âœ… |
| **StochRsi**   | 283.53x      | 4.55x        | **98% faster** âœ… |
| **Cmo**        | 257.78x      | 2.72x        | **99% faster** âœ… |
| **Chandelier** | 121.65x      | 5.35x        | **96% faster** âœ… |
| **Stoch**      | 15.69x       | 3.24x        | **79% faster** âœ… |
| **ForceIndex** | 61.56x       | 2.49x        | **96% faster** âœ… |
| **Slope**      | 7.49x        | 5.75x        | **23% faster** âœ… |

These indicators now use proper O(1) incremental updates instead of O(n) recalculations.

### StreamHub Recommendations

1. **53% of implementations meet target or acceptable ranges**
2. **All O(nÂ²) complexity issues resolved**
3. **EMA/SMA family overhead acceptable** for real-time use cases (~40,000 quotes/sec)
4. Monitor indicators in 3x-10x range for optimization opportunities
5. Framework overhead investigation (P001) deferred to future work

## Memory Overhead Analysis

### Memory Validation Status

Infrastructure ready, baseline data collection pending execution.

### Memory Profiling Infrastructure

**BenchmarkDotNet Configuration:**

- MemoryDiagnoser enabled with generational GC columns in PerformanceConfig
- Tracks allocated bytes, Gen0/Gen1/Gen2 collections per 1,000 operations
- Results sorted from fastest to slowest with rank column
- Measurements include instance overhead, internal state, result storage

**Measurement Scope:**

- Instance creation overhead
- Internal state storage (buffers, caches, windows)
- Result collection overhead
- Input quote data excluded (shared across all indicators)

**Test Scenarios:**

- 502 periods (standard dataset)
- Multiple indicator types (simple, complex, multi-series)
- All three styles for comparison

### Expected Memory Patterns

**Series Indicators (Baseline):**

- Result collection: ~40 bytes per period
- For 502 periods: ~20KB total
- No persistent state

**BufferList Indicators:**

- Internal buffer: Depends on lookback period
  - 14-period SMA: ~112 bytes
  - 20-period EMA: ~160 bytes
- Expected overhead: <5KB beyond result storage
- Target: âœ… Should meet <5KB easily

**StreamHub Indicators:**

- Provider cache: ~8 bytes per period for references
- State variables:
  - Simple: <1KB
  - Complex: 2-5KB
  - Multi-series: <3KB
- Expected overhead: <10KB for most indicators
- Target: âœ… Should meet <10KB for properly implemented indicators

### Memory Profiling Execution

Generate memory baseline data:

```bash
cd tools/performance

# Run with memory diagnostics enabled
dotnet run -c Release

# Extract memory data
cat BenchmarkDotNet.Artifacts/results/Performance.StyleComparison-report-full.json | \
  jq '.Benchmarks[] | {
    Method: .Method,
    Mean: .Statistics.Mean,
    Allocated: .Memory.BytesAllocatedPerOperation,
    Gen0: .Memory.Gen0Collections,
    Gen1: .Memory.Gen1Collections,
    Gen2: .Memory.Gen2Collections
  }'
```

### Memory Compliance Validation

Check compliance against NFR-002 targets:

```bash
# Check StreamHub compliance (<10KB overhead)
cat BenchmarkDotNet.Artifacts/results/Performance.StreamIndicators-report-full.json | \
  jq '.Benchmarks[] | select(.Memory.BytesAllocatedPerOperation > 10240) | 
    {method: .Method, allocated: .Memory.BytesAllocatedPerOperation}'

# Check BufferList compliance (<5KB overhead)
cat BenchmarkDotNet.Artifacts/results/Performance.BufferIndicators-report-full.json | \
  jq '.Benchmarks[] | select(.Memory.BytesAllocatedPerOperation > 5120) | 
    {method: .Method, allocated: .Memory.BytesAllocatedPerOperation}'
```

## Performance Regression Detection

### Automated Regression Detection

The `detect-regressions.ps1` script provides automated performance regression detection:

```bash
# Compare with specific baseline
pwsh tools/performance/detect-regressions.ps1 \
  -BaselineFile baselines/baseline-v3.0.0.json \
  -ThresholdPercent 10

# Auto-detect latest baseline and results
pwsh tools/performance/detect-regressions.ps1
```

### Regression Detection Criteria

By default, a performance regression is flagged when:

- Mean execution time increases by more than 10% compared to baseline
- Threshold configurable via `-ThresholdPercent` parameter

### CI/CD Integration

The `test-performance.yml` GitHub Actions workflow provides:

- Manual trigger for full benchmark suite execution
- Automated result publishing to GitHub Summary
- Artifact upload for historical tracking

## Analysis Tools

### Performance Analysis Script

Use the Python analysis script to generate detailed comparisons:

```bash
cd tools/performance/baselines
python3 analyze_performance.py
```

This script:

- Loads baseline JSON for Series, Stream, and Buffer styles
- Calculates performance ratios (Stream/Series, Buffer/Series)
- Flags indicators exceeding thresholds (>30% slower)
- Prints summary with optimization recommendations
- Identifies O(nÂ²) loops, unnecessary allocations, inefficient look-back operations

### Baseline Management

**Creating baselines:**

```bash
# After running benchmarks
cd tools/performance

# Copy results to baselines
cp BenchmarkDotNet.Artifacts/results/Performance.StyleComparison-report-full.json \
   baselines/baseline-v3.1.0.json

# Update latest baseline
cp baselines/baseline-v3.1.0.json baselines/baseline-latest.json

# Commit to repository
git add baselines/
git commit -m "perf: Update performance baselines for v3.1.0"
```

**Baseline management best practices:**

- Create baselines for each major/minor release
- Update baselines when intentional performance changes are made
- Keep at least the last 3 version baselines for historical comparison
- Document significant performance changes in release notes

## Quality Gates Status (Q002-Q006)

### Completed Tasks

- âœ… **Q002**: BufferList vs Series benchmarks exist and analyzed
  - StyleComparison benchmarks cover representative indicators
  - Baseline data available in `baselines/` directory
  - 93% meet acceptable performance targets

- âœ… **Q003**: StreamHub vs Series benchmarks exist and analyzed
  - StyleComparison benchmarks cover representative indicators
  - Baseline data available in `baselines/` directory
  - All O(nÂ²) issues resolved, 53% meet targets

- âœ… **Q004**: Memory overhead validation infrastructure ready
  - MemoryDiagnoser added to BenchmarkConfig
  - Ready for execution to generate memory baselines
  - Validation scripts documented

- âœ… **Q005**: Regression detection script operational
  - `detect-regressions.ps1` provides automated detection
  - Configurable thresholds
  - Ready for CI/CD integration

- âœ… **Q006**: Memory baseline structure defined
  - Documentation framework created in `baselines/memory/`
  - Collection methodology established
  - Compliance validation approach defined

### Next Steps

1. Execute benchmarks with memory diagnostics to populate memory baseline data
2. Create memory baseline files following structure in `baselines/memory/`
3. Validate memory compliance against NFR-002 targets
4. Document memory profiling findings
5. Consider optimization work for BufferList critical cases (Slope, Alligator, Adx)

## References

- Main benchmarking guide: `benchmarking.md`
- Streaming plan: `docs/plans/streaming-indicators.plan.md`
- Baseline management: `baselines/README.md`
- Memory baselines: `baselines/memory/README.md`
- Regression detection: `detect-regressions.ps1`
- Performance analysis: `baselines/analyze_performance.py`
- GitHub Actions workflow: `.github/workflows/test-performance.yml`
- NFR-002: Performance and memory requirements for streaming indicators

---

Last updated: January 3, 2026
