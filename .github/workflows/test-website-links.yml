# Checks website for broken links
name: Website Links

on:
  pull_request:
    paths:
      - docs/**
      - .github/workflows/test-website-links.yml

# Prevent multiple concurrent runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: docs
    steps:
      - name: Checkout source
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install linkchecker

      - name: Use 'localhost'
        uses: jacobtomlinson/gha-find-replace@f1069b438f125e5395d84d1c6fd3b559a7880cb5
        with:
          find: "https://dotnet.stockindicators.dev"
          replace: "http://127.0.0.1:8000"
          regex: false
          include: "docs/**"

      - name: Copy non-production robots.txt
        run: cp website/robots_nonprod.txt website/robots.txt

      - name: Build site
        env:
          GOOGLE_ANALYTICS_KEY: ""
        run: mkdocs build

      - name: Serve site
        run: mkdocs serve --no-livereload &

      # Wait for the server to start
      - name: Wait for server
        run: sleep 5

      # Test for broken URLs
      - name: Test for broken URLs
        run: |
          echo "Starting link check..."
          # Run linkchecker with only internal link checking
          if linkchecker --no-robots --output text http://127.0.0.1:8000; then
            echo "✅ Link check passed: No broken links found"
          else
            exit_code=$?
            echo "❌ Link check failed with code ${exit_code}: Found broken links"
            echo "See detailed report in the log above"
            exit 1
          fi

      - name: Kill site (failsafe)
        if: always()
        run: pkill -f mkdocs || true
